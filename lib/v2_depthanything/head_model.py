#!/usr/bin/env python3
# -*- coding: utf-8 -*-


# ---------------------------------------------------------------------------------------------------------------------
# %% Imports

import torch.nn as nn

from .components.misc_helpers import SpatialUpsampleLayer, Conv3x3Layer, Conv1x1Layer

# For type hints
from torch import Tensor


# ---------------------------------------------------------------------------------------------------------------------
# %% Classes


class MonocularDepthHead(nn.Module):
    """
    Implementation of the 'Monocular depth estimate head' model, described in:
        "Vision Transformers for Dense Prediction"
        By: RenÃ© Ranftl, Alexey Bochkovskiy, Vladlen Koltun
        @ https://arxiv.org/abs/2103.13413

    This model is a multi-stage convolution model, whose main purpose is to reduce the number of
    channels of the given input down to 1, representing the final depth-estimate value from
    a DPT (dense prediction transformer) model and restore the original input resolution.

    For more info, see the appendix of the paper, section: "Monocular depth estimation head"

    This implementation has modifications specific to the Depth-Anything model,
    which does not simply scale the output by a factor of 2 like the original DPT models.
    The original Depth-Anything implementation scales the result to a target size
    (which is taken as an input), though the implementation here does something different...

    In this implementation, the scaling is fixed at a factor of P/8, where P is the
    patch size used by the patch embedding (14 by default).
    To see why this works, consider that the DPT model takes an input image of
    size N and divides it into N/P patches. The fusion component of the model
    ends up scaling this patch sizing by a factor of 2*2*2, and then the
    head model (this module) is meant to restore the sizing back to the original
    input size. So we need a factor H which multiplies the sizing from the fusion
    step back to the original size N:

        H * (2*2*2) * (N/P) = N
        H * 8 / P = 1

        So: H = P / 8

    This approach is more fragile than the original Depth-Anything version, but allows
    the model to share an implementation with existing DPT models, so is preferred here.

    Also note that the V2 metric models use a slightly modified head model, which
    is supported by this implementation (though this isn't part of the original DPT structure).
    """

    # .................................................................................................................

    def __init__(self, num_channels_in: int, patch_size_px: int = 14, is_metric: bool = False):

        # Inherit from parent
        super().__init__()

        # For clarity, define layer config settings
        spatial_upsample_factor = patch_size_px / 8
        channels_in = num_channels_in
        channels_half = num_channels_in // 2
        channels_fixed = 32
        channels_out = 1

        # Create component that spatially upsamples input features (while reducing channels)
        self.spatial_upsampler = nn.Sequential(
            Conv3x3Layer(channels_in, channels_half),
            SpatialUpsampleLayer(scale_factor=spatial_upsample_factor),
        )

        # Create component which collapses features to a single channel
        self.proj_1ch = nn.Sequential(
            Conv3x3Layer(channels_half, channels_fixed),
            nn.ReLU(True),
            Conv1x1Layer(channels_fixed, channels_out),
            nn.ReLU(True) if not is_metric else nn.Sigmoid(),
        )

    # .................................................................................................................

    def forward(self, imagelike_bchw: Tensor) -> Tensor:
        """
        Spatially upscales the input image-like tokens while projecting down
        to a single channel.

        The input is expected to have shape: BxCxHxW
        The output has shape: Bx1xH'xW'
        -> Where H', W' are the upscaled height & width (factor of 1.75x by default)
        """

        # Halve channels while upsampling spatially
        output = self.spatial_upsampler(imagelike_bchw)

        # Collapse down to 1 channel for depth output
        output = self.proj_1ch(output)

        # Remove unitary channel for output (shape goes from: Bx1xHxW -> BxHxW)
        return output.squeeze(dim=1)

    # .................................................................................................................
